{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcf6e57-1c59-44c3-8710-a3fa927b4046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-21 17:24:11 [__init__.py:248] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, load_from_disk, concatenate_datasets\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0107fb4c-5a46-4dbb-8774-209b71168555",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = 2\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fdeca6-d6b4-4d2f-a438-6394da58945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('amang1802/wikipedia_controversial_sections')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce814c2-8a2c-4084-9559-e3dc66896d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.filter(lambda row: row['issue_scale'] > 0 and len(row['section_text'].strip()) > 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7950f9-8ff3-40f4-b059-da2796d0a2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article_title', 'url', 'section_title', 'section_text', 'classification_json', 'issue_scale'],\n",
       "    num_rows: 175575\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5078a1c4-c45f-4ff8-9c80-a0e6b96a5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-27b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe7b2fc-fd4e-479a-8cf7-01b1b8579e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a56b4b-5c8c-4f2f-ac1f-9c68337d7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-21 17:24:14 [__init__.py:30] Available plugins for group vllm.general_plugins:\n",
      "INFO 06-21 17:24:14 [__init__.py:32] name=lora_filesystem_resolver, value=vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 06-21 17:24:14 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.\n",
      "INFO 06-21 17:24:14 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.\n",
      "INFO 06-21 17:24:14 [__init__.py:44] plugin lora_filesystem_resolver loaded.\n",
      "INFO 06-21 17:24:20 [config.py:787] This model supports multiple tasks: {'reward', 'score', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 06-21 17:24:20 [config.py:1869] Defaulting to use mp for distributed inference\n",
      "INFO 06-21 17:24:20 [config.py:2112] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 06-21 17:24:22 [core.py:427] Waiting for init message from front-end.\n",
      "INFO 06-21 17:24:22 [core.py:61] Initializing a V1 LLM engine (v0.9.1.dev1+g258bf621d.d20250519) with config: model='google/gemma-3-27b-it', speculative_config=None, tokenizer='google/gemma-3-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=17408, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "WARNING 06-21 17:24:22 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-21 17:24:22 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_122b30a7'), local_subscribe_addr='ipc:///tmp/42b1fd0f-e388-4d7a-b40a-e26ca73545e7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-21 17:24:23 [utils.py:2664] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ce2981a9fc0>\n",
      "WARNING 06-21 17:24:23 [utils.py:2664] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ce2981aa2f0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:23 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5f55f295'), local_subscribe_addr='ipc:///tmp/84b46acc-83d3-4c15-97cb-5012e132f2bc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:24:23 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e38fe623'), local_subscribe_addr='ipc:///tmp/04552cf5-76f2-43ee-bfa7-186e3a6083bb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W621 17:24:24.413613725 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W621 17:24:24.741971261 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W621 17:24:25.177376607 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W621 17:24:25.391554101 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W621 17:24:25.391920084 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:26 [utils.py:1071] Found nccl from library libnccl.so.2\n",
      "INFO 06-21 17:24:26 [utils.py:1071] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:26 [pynccl.py:69] vLLM is using nccl==2.26.2\n",
      "INFO 06-21 17:24:26 [pynccl.py:69] vLLM is using nccl==2.26.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W621 17:24:26.821802419 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W621 17:24:26.822090599 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:27 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 06-21 17:24:27 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:27 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_6ae4c5f4'), local_subscribe_addr='ipc:///tmp/10d85176-3e8a-4888-80b5-3cc7f04e83c3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:27 [parallel_state.py:1079] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 06-21 17:24:27 [parallel_state.py:1079] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m WARNING 06-21 17:24:32 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:32 [gpu_model_runner.py:1503] Starting to load model google/gemma-3-27b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:32 [cuda.py:216] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m WARNING 06-21 17:24:32 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:24:32 [gpu_model_runner.py:1503] Starting to load model google/gemma-3-27b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:32 [backends.py:37] Using InductorAdaptor\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:24:32 [cuda.py:216] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:24:32 [backends.py:37] Using InductorAdaptor\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:24:32 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:32 [weight_utils.py:291] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf88d5820a434351a333f04c46ee1b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:24:40 [default_loader.py:279] Loading weights took 7.34 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:40 [default_loader.py:279] Loading weights took 7.38 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:24:40 [gpu_model_runner.py:1521] Model loading took 25.9044 GiB and 7.692331 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:40 [gpu_model_runner.py:1521] Model loading took 25.9044 GiB and 8.068762 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:24:40 [gpu_model_runner.py:1823] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 64 image items of the maximum feature size.\n",
      "INFO 06-21 17:24:40 [gpu_model_runner.py:1823] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 64 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:55 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/4e6b6db515/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:24:55 [backends.py:469] Dynamo bytecode transform time: 11.96 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:24:55 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/4e6b6db515/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:24:55 [backends.py:469] Dynamo bytecode transform time: 11.99 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:25:06 [backends.py:134] Directly load the compiled graph(s) for shape None from the cache, took 9.999 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:25:06 [backends.py:134] Directly load the compiled graph(s) for shape None from the cache, took 10.026 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:25:11 [monitor.py:33] torch.compile takes 11.99 s in total\n",
      "INFO 06-21 17:25:11 [monitor.py:33] torch.compile takes 11.96 s in total\n",
      "INFO 06-21 17:25:12 [kv_cache_utils.py:637] GPU KV cache size: 365,664 tokens\n",
      "INFO 06-21 17:25:12 [kv_cache_utils.py:640] Maximum concurrency for 17,408 tokens per request: 21.01x\n",
      "INFO 06-21 17:25:12 [kv_cache_utils.py:637] GPU KV cache size: 365,664 tokens\n",
      "INFO 06-21 17:25:12 [kv_cache_utils.py:640] Maximum concurrency for 17,408 tokens per request: 21.01x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:25:42 [custom_all_reduce.py:195] Registering 8308 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:25:42 [custom_all_reduce.py:195] Registering 8308 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=806528)\u001b[0;0m INFO 06-21 17:25:42 [gpu_model_runner.py:1890] Graph capturing finished in 30 secs, took 1.13 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=806527)\u001b[0;0m INFO 06-21 17:25:42 [gpu_model_runner.py:1890] Graph capturing finished in 30 secs, took 1.13 GiB\n",
      "INFO 06-21 17:25:42 [core.py:163] init engine (profile, create kv cache, warmup model) took 61.78 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=17408, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8196360-e59b-4ce2-bbcd-c012f0ae1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "# Task:\n",
    "Generate Interview Questions for Opposing Viewpoints on a Divergent Issue\n",
    "\n",
    "# Instructions\n",
    "You will be given:\n",
    "\n",
    "- An excerpt from a Wikipedia article (title, section, and text)\n",
    "- A divergent_issue_rationale that identifies the core disagreement or debate\n",
    "\n",
    "Your task is to create two \"Explain why...\" questions that would elicit responses from people holding opposing viewpoints on exactly one issue. If there is more than one issue, pick only one. These questions should be designed for interviews to understand each side's perspective.\n",
    "\n",
    "## Guidelines for Creating Questions:\n",
    "\n",
    "- Identify the two main opposing positions from the text and rationale. Ensure they are about the same issue.\n",
    "- Frame questions neutrally - avoid loaded language that suggests one side is right.\n",
    "- Make questions open-ended to encourage detailed explanations.\n",
    "- Focus on reasoning and values - questions should probe WHY people hold their views.\n",
    "- Ensure questions are answerable by someone holding that viewpoint.\n",
    "- Ensure at least one specific detail like name or place from the original text is included in the questions.\n",
    "- Include the same details in both for and against questions.\n",
    "- Keep the issue type consistent in both for and against questions, even if the text has multiple issues.\n",
    "- It's OK to ignore some issues if the text refers to multiple issues.\n",
    "- Include the general theme of the issue in the questions.\n",
    "\n",
    "## Question Structure:\n",
    "\n",
    "- Start with \"Explain why...\"\n",
    "- Reference the specific position or viewpoint\n",
    "- Ask about reasoning, values, or priorities behind that position\n",
    "- Avoid yes/no formulations\n",
    "\n",
    "# Output Format:\n",
    "Respond with a JSON object containing exactly these fields:\n",
    "\n",
    "{\n",
    "  \"specific_divergent_issue\": \"[One line description of the specific chosen issue]\",\n",
    "  \"for_description\": \"[Brief description of the first viewpoint/position]\",\n",
    "  \"for_question\": \"Explain why [specific question for holders of the first viewpoint/position]\",\n",
    "  \"against_description\": \"[Brief description of the opposing viewpoint/position]\", \n",
    "  \"against_question\": \"Explain why [specific question for holders of opposing viewpoint/position]\"\n",
    "}\n",
    "\n",
    "# Example:\n",
    "\n",
    "## Input\n",
    "Title: Heavy horse (General)\n",
    "\n",
    "Section: Controversy and social acceptance\n",
    "\n",
    "Text: Raising horses for meat is controversial, with many people opposed to hippophagy, and to the very idea of breeding horses for this purpose. According to Bernadette Lizet, in France, the heavy horse breeders present at the Salon International de l'Agriculture have taken to hiding their motivations from Parisian visitors, citing \"passion\", without ever mentioning the competition criteria for heavy breeds, or fattening workshops, or \"finishing-meat in Italy\". City dwellers are indeed shocked by the existence of such breeding, which they readily associate with \"barbarism\". Jean-Pierre Digard cites the example of a \"breeder of splendid heavy horses paralyzed by the fear of having to explain that he was raising horses for slaughter\", during an interview on a presentation podium at the 2008 Salon International de l'Agriculture. In the Spanish Basque Country, the marketing of locally-bred foal meat is based on an elaborate commercial strategy. The language is modernized to lessen the emotional impact created by the idea of consuming horse, speaking instead of \"foal meat\" (Carne de potro), the emotional impact of the word \"foal\" not being deemed as strong as that of the word \"horse\". Other controversies concern the abuse that some breeders inflict on heavy foals, and the awarding of breeding premiums to animals in poor health (obese, even lame) to the detriment of working horses, particularly in the Breton and Comtoise breeds.\n",
    "\n",
    "Issue: This text discusses a **divergent issue**: the ethical and social acceptability of raising horses for meat (hippophagy). The text clearly outlines differing viewpoints – those who oppose it based on ethical concerns and those who engage in the practice, attempting to mitigate negative perceptions. The breeders' attempts to conceal their motivations and the use of euphemistic language demonstrate a clear conflict in values and social acceptance.\n",
    "\n",
    "## Response\n",
    "{\n",
    "  \"specific_divergent_issue\": \"The acceptance of the practice of eating horse meat.\",\n",
    "  \"against_description\": \"Those who raise horses for meat, particularly heavy horse breeders, participate in the practice as a means of livelihood and potentially due to competition criteria within the industry. They attempt to navigate social perceptions by downplaying the association with 'horse' and focusing on terms like 'foal meat'.\",\n",
    "  \"against_question\": \"Explain why, despite the shock of some people like Parisian city dwellers, you believe raising horses for meat is a legitimate and ethical practice and can be justified as a cultural practice, and what steps you take to ensure the welfare of the animals throughout their lives.\",\n",
    "  \"for_description\": \"Those who oppose raising horses for meat believe it is ethically wrong, associating it with 'barbarism' and expressing emotional distress at the idea of consuming horse meat, particularly foals. They are concerned about animal welfare and the potential for abuse.\",\n",
    "  \"for_question\": \"Explain why you believe people are right to be shocked by the practice of hippophagy and that horses should be treated differently from other livestock animals commonly used for food, and what specific aspects of raising horses for meat you find most concerning.\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8796d341-a426-4e07-9b37-56c6f39b4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_section(title, section, text, reasoning):\n",
    "    return f\"Title: {title}\\n\\nSection: {section}\\n\\nText: {text}\\n\\nIssue:\\n\\n{reasoning}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bfe176e-8851-4cae-a207-b72246aaef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(response):\n",
    "    try:\n",
    "        data = json.loads(response)\n",
    "        for_question = data['for_question']\n",
    "        against_question = data['against_question']\n",
    "    except (ValueError, KeyError) :\n",
    "        for_question = \"\"\n",
    "        against_question = \"\"\n",
    "\n",
    "    return data, for_question, against_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "03d96e4d-11df-4605-8616-05c2b8f7044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_JSON_SCHEMA = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"specific_divergent_issue\": {\n",
    "        \"type\": \"string\"\n",
    "    },\n",
    "    \"for_description\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"for_question\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"against_description\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"against_question\": {\n",
    "      \"type\": \"string\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\n",
    "    \"specific_divergent_issue\",\n",
    "    \"for_description\",\n",
    "    \"for_question\",\n",
    "    \"against_description\",\n",
    "    \"against_question\"\n",
    "  ],\n",
    "  \"additionalProperties\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be81ccc6-6107-4b54-bf44-5749ea2d684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(titles, sections, texts, reasonings):\n",
    "    messages = [[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": format_section(title, section, text, reasoning['divergent_issue_rationale'])}]\n",
    "                for title, section, text, reasoning in zip(titles, sections, texts, reasonings)]\n",
    "    \n",
    "    outputs = llm.chat(messages, SamplingParams(temperature=0.6, top_p=0.9, max_tokens=512, guided_decoding=GuidedDecodingParams(json=OUTPUT_JSON_SCHEMA)))\n",
    "    responses = [output.outputs[0].text.strip() for output in outputs]\n",
    "    datas, for_questions, against_questions =  zip(*[extract_questions(response) for response in responses])\n",
    "    \n",
    "    return {\"questions_json\": list(datas), \"for_question\": list(for_questions), \"against_question\": list(against_questions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3ebd8-3b3c-40c0-9fa8-c4810f10cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da805c14bd7437ba45534c5e6fcf6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db88230f75464b20a9c9126adda4afd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/7023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf29bd41832424382918bd8a84908e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/7023 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_count = ds.num_rows\n",
    "num_steps = 25\n",
    "step_size = math.ceil(total_count / num_steps)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    print(f\"Running step: {step}\")\n",
    "\n",
    "    start_i = step * step_size\n",
    "    end_i = (step+1) * step_size\n",
    "    \n",
    "\n",
    "    cls_ds = ds.select(range(start_i, min(end_i, ds.num_rows))).map(generate_questions, batched=True, batch_size=step_size, input_columns=['article_title', 'section_title', 'section_text', 'classification_json'])\n",
    "    cls_ds.save_to_disk(f\"questions-test-{step}.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206adcdf-1d50-43f9-adf3-5dcab3082cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(\"questions-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4440f98-5e08-4c55-849b-930e3db6376b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['questions-0.hf',\n",
       " 'questions-1.hf',\n",
       " 'questions-2.hf',\n",
       " 'questions-3.hf',\n",
       " 'questions-4.hf',\n",
       " 'questions-5.hf',\n",
       " 'questions-6.hf',\n",
       " 'questions-7.hf',\n",
       " 'questions-8.hf',\n",
       " 'questions-9.hf',\n",
       " 'questions-10.hf',\n",
       " 'questions-11.hf',\n",
       " 'questions-12.hf',\n",
       " 'questions-13.hf',\n",
       " 'questions-14.hf',\n",
       " 'questions-15.hf',\n",
       " 'questions-16.hf',\n",
       " 'questions-17.hf',\n",
       " 'questions-18.hf',\n",
       " 'questions-19.hf',\n",
       " 'questions-20.hf',\n",
       " 'questions-21.hf',\n",
       " 'questions-22.hf',\n",
       " 'questions-23.hf',\n",
       " 'questions-24.hf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f48b1e-ade1-439a-a71d-1e0719133995",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ds = []\n",
    "for f in files:\n",
    "    processed_ds.append(load_from_disk(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d8640d-7e9d-4a86-aa20-3830bf217578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175575"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ds = concatenate_datasets(processed_ds)\n",
    "combined_ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b61c52-ffd7-4974-bfbf-2d90713339da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_title': 'Heavy horse (General)',\n",
       " 'url': 'https://en.wikipedia.org/wiki/Heavy_horse_(General)',\n",
       " 'section_title': 'Controversy and social acceptance',\n",
       " 'section_text': 'Raising horses for meat is controversial, with many people opposed to hippophagy, and to the very idea of breeding horses for this purpose. According to Bernadette Lizet, in France, the heavy horse breeders present at the Salon International de l\\'Agriculture have taken to hiding their motivations from Parisian visitors, citing \"passion\", without ever mentioning the competition criteria for heavy breeds, or fattening workshops, or \"finishing-meat in Italy\". City dwellers are indeed shocked by the existence of such breeding, which they readily associate with \"barbarism\". Jean-Pierre Digard cites the example of a \"breeder of splendid heavy horses paralyzed by the fear of having to explain that he was raising horses for slaughter\", during an interview on a presentation podium at the 2008 Salon International de l\\'Agriculture. In the Spanish Basque Country, the marketing of locally-bred foal meat is based on an elaborate commercial strategy. The language is modernized to lessen the emotional impact created by the idea of consuming horse, speaking instead of \"foal meat\" (Carne de potro), the emotional impact of the word \"foal\" not being deemed as strong as that of the word \"horse\". Other controversies concern the abuse that some breeders inflict on heavy foals, and the awarding of breeding premiums to animals in poor health (obese, even lame) to the detriment of working horses, particularly in the Breton and Comtoise breeds.',\n",
       " 'classification_json': {'divergent_issue_rationale': \"This text discusses a **divergent issue**: the ethical and social acceptability of raising horses for meat (hippophagy). The text clearly outlines differing viewpoints – those who oppose it based on ethical concerns and those who engage in the practice, attempting to mitigate negative perceptions. The breeders' attempts to conceal their motivations and the use of euphemistic language demonstrate a clear conflict in values and social acceptance.\",\n",
       "  'divergent_issue_scale': 3,\n",
       "  'divergent_issue_scale_rationale': \"The scale is rated a 3. While not a globally recognized issue, the controversy surrounding horse meat consumption is present in several regions (France, Spain, and specifically the Basque Country, and within specific breeding communities). It affects identifiable stakeholder groups: horse breeders, animal welfare advocates, consumers, and those with cultural or emotional attachments to horses. The issue has regional implications and sparks debate within those communities, but doesn't reach the level of a national or global crisis.\",\n",
       "  'is_divergent_issue': True},\n",
       " 'issue_scale': 3,\n",
       " 'questions_json': {'against_description': \"Those who raise horses for meat, particularly heavy horse breeders, participate in the practice as a means of livelihood and potentially due to competition criteria within the industry. They attempt to navigate social perceptions by downplaying the association with 'horse' and focusing on terms like 'foal meat'.\",\n",
       "  'against_question': \"Explain why, given the breeder in the text who was 'paralyzed by the fear of having to explain' raising horses for slaughter, a heavy horse breeder at the Salon International de l'Agriculture might choose to emphasize 'passion' over explicitly stating the competition criteria and 'finishing-meat in Italy' as their primary motivations?\",\n",
       "  'for_description': \"Those who oppose raising horses for meat believe it is ethically wrong, associating it with 'barbarism' and expressing emotional distress at the idea of consuming horse meat, particularly foals. They are concerned about animal welfare and the potential for abuse.\",\n",
       "  'for_question': \"Explain why, considering the emotional reaction of Parisian visitors at the Salon International de l'Agriculture as described in the text, someone would find raising heavy horses like the Breton or Comtoise breeds specifically for slaughter to be ethically unacceptable, even if the breeders claim 'passion' as their motivation?\"},\n",
       " 'for_question': \"Explain why, considering the emotional reaction of Parisian visitors at the Salon International de l'Agriculture as described in the text, someone would find raising heavy horses like the Breton or Comtoise breeds specifically for slaughter to be ethically unacceptable, even if the breeders claim 'passion' as their motivation?\",\n",
       " 'against_question': \"Explain why, given the breeder in the text who was 'paralyzed by the fear of having to explain' raising horses for slaughter, a heavy horse breeder at the Salon International de l'Agriculture might choose to emphasize 'passion' over explicitly stating the competition criteria and 'finishing-meat in Italy' as their primary motivations?\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4260f5-0139-4643-95e9-1f1f11308de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most notable incidents that occurred regarding the magazine was an unauthorized adaptation of Harlan Ellison 's short story, \"A Boy and His Dog\", which has been rumored as one of the major factors in the bankruptcy of Warren Publishing. As discussed in the book The Warren Companion, editor Bill Dubay approached writers Gerry Boudreau and Jim Stenstrum about adapting science fiction stories for the magazine. Boudreau asked permission to adapt Ellison's story, and Dubay approved this, without first asking Ellison. When Ellison refused to grant permission, Dubay had artist Alex Niño draw the story anyway, then provided the art to Stenstrum to use as the basis for a new story. The story was published in issue #4, under the title \"Mondo Megillah\". Despite Stenstrum's reworking of the script, the basic story was still obvious plagiarism and Ellison filed a lawsuit, which he eventually won. Advertised as an adult fantasy magazine, 1984 contained very mature subject matter by the standards of the time. To compete with rivals such as Heavy Metal it contained many stories featuring sex and other controversial subjects. As discussed by comics historian Richard Arndt, editor DuBay edited stories within the magazine to focus more on this subject matter, such as this incident that occurred with artist Wally Wood regarding stories that appeared in the first two issues of the magazine: Wood's original story was entitled 'The End' and was 12 pages long. It was a part of his Wizard King series. Bill DuBay, without Wood's o.k. or knowledge, split the story in two, rearranged pages & panels, rewrote Wood's original script and presented the greatly altered work as two separate stories, changing Wood's original rather charming adult oriented tale into shorter pieces that leaned heavily on the scenes (which were also in Wood's original but not nearly so highlighted as their appearance here) of naked women in bondage being whipped and brutalized. Understandably, Wood was outraged and never worked for Warren again. DuBay's treatment of Corben and Strnad's Mutant World also alienated the creators. Throughout, DuBay altered Strnad's dialogue to include words and phrases that Strnad has called \"a spew of juvenile obscenities.\" The artwork, also, was altered as one page was arbitrarily flipped right-to-left, with the lettering adjusted to accommodate the change. When approached by Warren to publish an album of the collected episodes, Corben and Strnad politely declined. Controversial stories included issue #3's satirical story \"The Harvest\" which featured a future where white people hunted black people for sport and ate them, and issue #13's science fiction story \"The Crop\" where babies are sliced up and processed through factories to provide food for the starving populace. Both stories were written by DuBay. Despite its controversies, the magazine has been praised for the high quality of its art. The serials Young Sigmond Pavlov and Ghita of Alizarr were both singled out as high quality stories by David A. Roach in The Warren Companion.\n"
     ]
    }
   ],
   "source": [
    "print(combined_ds[0]['section_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196b7d7-7f0f-4790-bd15-057947fa8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_ds.push_to_hub('amang1802/wikipedia_controversial_abductive_alignment_prompts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9b190-d822-454d-9030-bc7a3976b866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
