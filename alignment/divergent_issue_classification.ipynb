{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbcf6e57-1c59-44c3-8710-a3fa927b4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, load_from_disk, concatenate_datasets\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0107fb4c-5a46-4dbb-8774-209b71168555",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = 2\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fdeca6-d6b4-4d2f-a438-6394da58945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('amang1802/wikipedia_controversial_sections')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7950f9-8ff3-40f4-b059-da2796d0a2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article_title', 'url', 'section_title', 'section_text'],\n",
       "    num_rows: 184160\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5078a1c4-c45f-4ff8-9c80-a0e6b96a5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-27b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe7b2fc-fd4e-479a-8cf7-01b1b8579e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc64e9f-85df-42a9-b44c-59cbe385cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb7622a-6268-4b20-9582-3d617723306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['section_length'] = df.apply(lambda row: len(row['section_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c53be52-498b-498d-aadf-e4a5fbaf22a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14316"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(df.iloc[df['section_length'].argmax()].section_text)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8196360-e59b-4ce2-bbcd-c012f0ae1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "# Task: Analyze Wikipedia Text for Divergent Issues\n",
    "You will be given an excerpt from a Wikipedia article with a title, section name, and text content. Your task is to determine if the text discusses a divergent issue and assess its scale.\n",
    "\n",
    "\n",
    "# Definition of a Divergent Issue:\n",
    "A divergent issue is a topic where:\n",
    "\n",
    "Different groups of people hold substantially different viewpoints, opinions, or positions\n",
    "There is legitimate debate, controversy, or disagreement about the topic\n",
    "The disagreement stems from different values, priorities, interpretations, or interests (not just factual errors)\n",
    "\n",
    "# Scoring Scale (1-5):\n",
    "\n",
    "0: Not a divergent issue at all\n",
    "1: Very niche issue with minimal community interest (affects only a tiny, specialized group)\n",
    "2: Limited issue affecting a small but identifiable community (local controversy, specialized field debate)\n",
    "3: Moderate issue with clear stakeholder groups (regional issue, specific industry concern, particular demographic)\n",
    "4: Reasonably well-known issue with broad community interest (national debates, major industry concerns, widespread social issues)\n",
    "5: Major societal issue with widespread recognition (global concerns, fundamental rights, major political/social divides)\n",
    "\n",
    "# Output Format:\n",
    "Respond with a JSON object containing exactly these fields:\n",
    "\n",
    "{\n",
    "  \"divergent_issue_rationale\": \"[Explain why this is or isn't a divergent issue, starting with 'This text discusses a **divergent issue**...' or 'This text does **not** contain discussion of a divergent issue...']\",\n",
    "  \"is_divergent_issue\": [true or false],\n",
    "  \"divergent_issue_scale_rationale\": \"[If divergent issue: explain the scale rating considering community size, geographic scope, and societal impact. If not divergent issue: exactly 'NA']\",\n",
    "  \"divergent_issue_scale\": [0-5, where 0 means not a divergent issue]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebe4de54-c96a-4cca-b474-5c568104dbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(system_prompt)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17c3b21a-fb39-4288-b326-2eac0aed3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VLLM_USE_FLASHINFER'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a56b4b-5c8c-4f2f-ac1f-9c68337d7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 19:19:55 [__init__.py:30] Available plugins for group vllm.general_plugins:\n",
      "INFO 06-17 19:19:55 [__init__.py:32] name=lora_filesystem_resolver, value=vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 06-17 19:19:55 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.\n",
      "INFO 06-17 19:19:55 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.\n",
      "INFO 06-17 19:19:55 [__init__.py:44] plugin lora_filesystem_resolver loaded.\n",
      "INFO 06-17 19:20:02 [config.py:787] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 06-17 19:20:02 [config.py:1869] Defaulting to use mp for distributed inference\n",
      "INFO 06-17 19:20:02 [config.py:2112] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 06-17 19:20:04 [core.py:427] Waiting for init message from front-end.\n",
      "INFO 06-17 19:20:04 [core.py:61] Initializing a V1 LLM engine (v0.9.1.dev1+g258bf621d.d20250519) with config: model='google/gemma-3-27b-it', speculative_config=None, tokenizer='google/gemma-3-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "WARNING 06-17 19:20:04 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-17 19:20:04 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_a99935d5'), local_subscribe_addr='ipc:///tmp/b8ffbe0a-8c59-4e5f-b761-4fe076eb9b36', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-17 19:20:04 [utils.py:2664] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7b2b294116f0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:04 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_93c49d23'), local_subscribe_addr='ipc:///tmp/999c0449-e778-4380-9c39-edd98a719f02', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-17 19:20:04 [utils.py:2664] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7b2b29412fb0>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:04 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_53aac81f'), local_subscribe_addr='ipc:///tmp/afb9fe5a-00b0-49da-9652-fa675d354340', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W617 19:20:06.177721447 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W617 19:20:06.250432617 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W617 19:20:06.250773476 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:06 [utils.py:1071] Found nccl from library libnccl.so.2\n",
      "INFO 06-17 19:20:06 [utils.py:1071] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:06 [pynccl.py:69] vLLM is using nccl==2.26.2\n",
      "INFO 06-17 19:20:06 [pynccl.py:69] vLLM is using nccl==2.26.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W617 19:20:06.461040835 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W617 19:20:06.461202404 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:07 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 06-17 19:20:07 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:07 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_5ac88801'), local_subscribe_addr='ipc:///tmp/30a31a38-59fd-4a80-a8d7-6fa8a0f0be5d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:07 [parallel_state.py:1079] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 06-17 19:20:07 [parallel_state.py:1079] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m WARNING 06-17 19:20:11 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:11 [gpu_model_runner.py:1503] Starting to load model google/gemma-3-27b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:11 [cuda.py:216] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m WARNING 06-17 19:20:11 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:11 [gpu_model_runner.py:1503] Starting to load model google/gemma-3-27b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:11 [backends.py:37] Using InductorAdaptor\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:11 [cuda.py:216] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:11 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:12 [backends.py:37] Using InductorAdaptor\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f4048c68124ce7a50bf35246dee19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:12 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:19 [default_loader.py:279] Loading weights took 7.28 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:19 [gpu_model_runner.py:1521] Model loading took 25.9044 GiB and 7.641194 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:20 [default_loader.py:279] Loading weights took 8.11 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:20 [gpu_model_runner.py:1521] Model loading took 25.9044 GiB and 8.529004 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:20 [gpu_model_runner.py:1823] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 64 image items of the maximum feature size.\n",
      "INFO 06-17 19:20:20 [gpu_model_runner.py:1823] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 64 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:34 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/fc73c6e44f/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:34 [backends.py:469] Dynamo bytecode transform time: 11.90 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:34 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/fc73c6e44f/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:34 [backends.py:469] Dynamo bytecode transform time: 11.95 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:20:45 [backends.py:134] Directly load the compiled graph(s) for shape None from the cache, took 10.047 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:45 [backends.py:134] Directly load the compiled graph(s) for shape None from the cache, took 10.102 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:20:51 [monitor.py:33] torch.compile takes 11.90 s in total\n",
      "INFO 06-17 19:20:51 [monitor.py:33] torch.compile takes 11.95 s in total\n",
      "INFO 06-17 19:20:52 [kv_cache_utils.py:637] GPU KV cache size: 412,912 tokens\n",
      "INFO 06-17 19:20:52 [kv_cache_utils.py:640] Maximum concurrency for 16,384 tokens per request: 25.20x\n",
      "INFO 06-17 19:20:52 [kv_cache_utils.py:637] GPU KV cache size: 412,912 tokens\n",
      "INFO 06-17 19:20:52 [kv_cache_utils.py:640] Maximum concurrency for 16,384 tokens per request: 25.20x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:21:20 [custom_all_reduce.py:195] Registering 8308 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:21:20 [custom_all_reduce.py:195] Registering 8308 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=757992)\u001b[0;0m INFO 06-17 19:21:20 [gpu_model_runner.py:1890] Graph capturing finished in 28 secs, took 1.13 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=757991)\u001b[0;0m INFO 06-17 19:21:20 [gpu_model_runner.py:1890] Graph capturing finished in 28 secs, took 1.13 GiB\n",
      "INFO 06-17 19:21:20 [core.py:163] init engine (profile, create kv cache, warmup model) took 59.59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=16384, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8796d341-a426-4e07-9b37-56c6f39b4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_section(title, section, text):\n",
    "    return f\"Title: {title}\\n\\nSection: {section}\\n\\nText: {text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bfe176e-8851-4cae-a207-b72246aaef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'Educational score: (\\d+)\\s*$'\n",
    "def extract_score(response):\n",
    "    try:\n",
    "        data = json.loads(response)\n",
    "        score = data.get('divergent_issue_scale')\n",
    "    except ValueError:\n",
    "        data = {}\n",
    "        score = 0\n",
    "\n",
    "    return data, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d96e4d-11df-4605-8616-05c2b8f7044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_JSON_SCHEMA = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"divergent_issue_rationale\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"is_divergent_issue\": {\n",
    "      \"type\": \"boolean\"\n",
    "    },\n",
    "    \"divergent_issue_scale_rationale\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"divergent_issue_scale\": {\n",
    "      \"type\": \"integer\",\n",
    "      \"minimum\": 0,\n",
    "      \"maximum\": 5\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\n",
    "    \"divergent_issue_rationale\",\n",
    "    \"is_divergent_issue\",\n",
    "    \"divergent_issue_scale_rationale\",\n",
    "    \"divergent_issue_scale\"\n",
    "  ],\n",
    "  \"additionalProperties\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be81ccc6-6107-4b54-bf44-5749ea2d684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(titles, sections, texts):\n",
    "    messages = [[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": format_section(title, section, text)}] for title, section, text in zip(titles, sections, texts)]\n",
    "    \n",
    "    outputs = llm.chat(messages, SamplingParams(temperature=0.25, max_tokens=512, guided_decoding=GuidedDecodingParams(json=OUTPUT_JSON_SCHEMA)))\n",
    "    responses = [output.outputs[0].text.strip() for output in outputs]\n",
    "    datas, scores =  zip(*[extract_score(response) for response in responses])\n",
    "    \n",
    "    return {\"classification_json\": list(datas), \"issue_scale\": list(scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4df3ebd8-3b3c-40c0-9fa8-c4810f10cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step: 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97954c0e255c4f5a9bf7e83e59ab696b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96404b46e5b04baf807f55d4647ecb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/7352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7f646e345249f6a04ab4564cfb428d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/7352 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f14df37d494a998a721777cb80f060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_count = ds.num_rows\n",
    "num_steps = 25\n",
    "step_size = math.ceil(total_count / num_steps)\n",
    "\n",
    "for step in range(24, num_steps):\n",
    "    print(f\"Running step: {step}\")\n",
    "\n",
    "    start_i = step * step_size\n",
    "    end_i = (step+1) * step_size\n",
    "    \n",
    "\n",
    "    cls_ds = ds.select(range(start_i, min(end_i, ds.num_rows))).map(classify, batched=True, batch_size=step_size, input_columns=['article_title', 'section_title', 'section_text'])\n",
    "    cls_ds.save_to_disk(f\"wiki-issue-{step}.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "206adcdf-1d50-43f9-adf3-5dcab3082cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(\"wiki-issue-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98f48b1e-ade1-439a-a71d-1e0719133995",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ds = []\n",
    "for f in files:\n",
    "    processed_ds.append(load_from_disk(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80d8640d-7e9d-4a86-aa20-3830bf217578",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds = concatenate_datasets(processed_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0320a2aa-0b37-4c74-9227-d1e2004fba45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184160"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21b61c52-ffd7-4974-bfbf-2d90713339da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_title': '1984 (magazine)',\n",
       " 'url': 'https://en.wikipedia.org/wiki/1984_(magazine)',\n",
       " 'section_title': 'Controversies',\n",
       " 'section_text': 'One of the most notable incidents that occurred regarding the magazine was an unauthorized adaptation of Harlan Ellison \\'s short story, \"A Boy and His Dog\", which has been rumored as one of the major factors in the bankruptcy of Warren Publishing. As discussed in the book The Warren Companion, editor Bill Dubay approached writers Gerry Boudreau and Jim Stenstrum about adapting science fiction stories for the magazine. Boudreau asked permission to adapt Ellison\\'s story, and Dubay approved this, without first asking Ellison. When Ellison refused to grant permission, Dubay had artist Alex Niño draw the story anyway, then provided the art to Stenstrum to use as the basis for a new story. The story was published in issue #4, under the title \"Mondo Megillah\". Despite Stenstrum\\'s reworking of the script, the basic story was still obvious plagiarism and Ellison filed a lawsuit, which he eventually won. Advertised as an adult fantasy magazine, 1984 contained very mature subject matter by the standards of the time. To compete with rivals such as Heavy Metal it contained many stories featuring sex and other controversial subjects. As discussed by comics historian Richard Arndt, editor DuBay edited stories within the magazine to focus more on this subject matter, such as this incident that occurred with artist Wally Wood regarding stories that appeared in the first two issues of the magazine: Wood\\'s original story was entitled \\'The End\\' and was 12 pages long. It was a part of his Wizard King series. Bill DuBay, without Wood\\'s o.k. or knowledge, split the story in two, rearranged pages & panels, rewrote Wood\\'s original script and presented the greatly altered work as two separate stories, changing Wood\\'s original rather charming adult oriented tale into shorter pieces that leaned heavily on the scenes (which were also in Wood\\'s original but not nearly so highlighted as their appearance here) of naked women in bondage being whipped and brutalized. Understandably, Wood was outraged and never worked for Warren again. DuBay\\'s treatment of Corben and Strnad\\'s Mutant World also alienated the creators. Throughout, DuBay altered Strnad\\'s dialogue to include words and phrases that Strnad has called \"a spew of juvenile obscenities.\" The artwork, also, was altered as one page was arbitrarily flipped right-to-left, with the lettering adjusted to accommodate the change. When approached by Warren to publish an album of the collected episodes, Corben and Strnad politely declined. Controversial stories included issue #3\\'s satirical story \"The Harvest\" which featured a future where white people hunted black people for sport and ate them, and issue #13\\'s science fiction story \"The Crop\" where babies are sliced up and processed through factories to provide food for the starving populace. Both stories were written by DuBay. Despite its controversies, the magazine has been praised for the high quality of its art. The serials Young Sigmond Pavlov and Ghita of Alizarr were both singled out as high quality stories by David A. Roach in The Warren Companion.',\n",
       " 'classification_json': {'divergent_issue_rationale': 'This text discusses a **divergent issue** regarding artistic integrity, creator rights, and editorial control within the comics industry. The controversies highlight disagreements between creators (Ellison, Wood, Corben, Strnad) and the editor (Dubay) over authorship, adaptation, and the alteration of artistic work. These disagreements stem from differing values regarding respect for creators, artistic vision, and the ethical boundaries of editorial intervention. The inclusion of controversial content like racially charged satire and graphic violence also points to differing views on acceptable subject matter.',\n",
       "  'divergent_issue_scale': 3,\n",
       "  'divergent_issue_scale_rationale': \"The scale is rated a 3. While the issues are specific to the comics industry and a relatively small group of creators and fans, they touch upon broader themes of artistic freedom and creator rights, which are relevant to a wider audience. The controversies surrounding the magazine and its editor were significant enough to contribute to the bankruptcy of the publisher, indicating a moderate level of impact. It's not a global issue, but it's more than a niche debate.\",\n",
       "  'is_divergent_issue': True},\n",
       " 'issue_scale': 3}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6196b7d7-7f0f-4790-bd15-057947fa8649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f5a23c7a7546988aac473f126eec50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d5cc76ee544d24a3ce0b8dcf29692a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87ff09c8a78438484a28a085c36bb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/lfs.py:337: UserWarning: hf_transfer is enabled but does not support uploading from bytes or BinaryIO, falling back to regular upload\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3e547fe425428991e8008501e87060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc72ca2244044cb986a3097702cb35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/amang1802/wikipedia_controversial_sections/commit/d232eef82c833816ba96d8e6e752948727cac9f5', commit_message='Upload dataset', commit_description='', oid='d232eef82c833816ba96d8e6e752948727cac9f5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/amang1802/wikipedia_controversial_sections', endpoint='https://huggingface.co', repo_type='dataset', repo_id='amang1802/wikipedia_controversial_sections'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ds.push_to_hub('amang1802/wikipedia_controversial_sections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9b190-d822-454d-9030-bc7a3976b866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
