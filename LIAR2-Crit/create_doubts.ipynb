{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86db6785-2d73-471e-b709-6450b174d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2add563c-7125-43ed-a566-90a1fc2ba873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from pydantic import BaseModel, TypeAdapter\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "import json\n",
    "import random\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e016f04-5a40-4355-825c-a8f7870069f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = 4\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8c3a1b-b200-49f0-85d2-888ed994988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "# Instructions\n",
    "\n",
    "Simon loves Socrates and never takes any statement at face value. Unfortunately, that sometimes mean that nothing convinces him of the truth of any statement. \n",
    "\n",
    "- Your goal is to impersonate Simon.\n",
    "- Assume all statements are false and made with a malicious intent to spread misinformation.\n",
    "- Output as many as possible reasons why he might question its truth.\n",
    "- These explanations are intelligent and rational.\n",
    "- They are intended to motivate research to answer the truth.\n",
    "- Keep these doubts short.\n",
    "- Make sure any two doubts are different from each other. We don't want similar doubts appearing multiple times.\n",
    "\n",
    "# Output Instructions\n",
    "\n",
    "Reply in JSON using the following format.\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"doubt\": \"string\"\n",
    "  },\n",
    "  {\n",
    "    \"doubt\": \"string\"\n",
    "  },\n",
    "  {\n",
    "    \"doubt\": \"string\"\n",
    "  },\n",
    "...\n",
    "]\n",
    "\n",
    "# Example:\n",
    "\n",
    "Speaker:\n",
    "Kathleen Sullivan is the town of Warren’s substance abuse coordinator. She is also program director for The BAY Team, Barrington’s Prevention Coalition.\n",
    "\n",
    "Context:\n",
    "testimony before the House Judiciary Committee\n",
    "\n",
    "Date:\n",
    "April 29, 2015\n",
    "\n",
    "Statement:\n",
    "The proportion of Rhode Islanders entering substance abuse treatment primarily due to marijuana use has reached its highest point in 20 years.\n",
    "\n",
    "Response:\n",
    "[\n",
    "  {\n",
    "    \"doubt\": \"Marijuana is legal now, and there are lots of studies which show that actually reduces substance abuse.\"\n",
    "  },\n",
    "  {\n",
    "    \"doubt\": \"While some drugs and opioids continue to cause a substance abuse problem, Marijuana substance has likely already peaked.\"\n",
    "  },\n",
    "  {\n",
    "    \"doubt\": \"It's a play on words and the truth maybe that it peaked sometime in the last 20 years.\"\n",
    "  }\n",
    "]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d4947d-52f8-43ae-a920-5d4e0373690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177c74e1-107b-4356-9485-8f5a928492e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 17:13:44 config.py:905] Defaulting to use mp for distributed inference\n",
      "INFO 01-23 17:13:44 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 01-23 17:13:44 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 01-23 17:13:44 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m INFO 01-23 17:13:44 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m INFO 01-23 17:13:44 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 01-23 17:13:44 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 01-23 17:13:49 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 01-23 17:13:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m INFO 01-23 17:13:49 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 01-23 17:13:49 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 01-23 17:13:49 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m INFO 01-23 17:13:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 01-23 17:13:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 01-23 17:13:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 01-23 17:13:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m INFO 01-23 17:13:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 01-23 17:13:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 01-23 17:13:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 01-23 17:13:51 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7bbf589c0d60>, local_subscribe_port=57187, remote_subscribe_port=None)\n",
      "INFO 01-23 17:13:51 model_runner.py:1056] Starting to load model meta-llama/Llama-3.3-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m INFO 01-23 17:13:51 model_runner.py:1056] Starting to load model meta-llama/Llama-3.3-70B-Instruct...\n",
      "INFO 01-23 17:13:51 model_runner.py:1056] Starting to load model meta-llama/Llama-3.3-70B-Instruct...\n",
      "INFO 01-23 17:13:51 model_runner.py:1056] Starting to load model meta-llama/Llama-3.3-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m INFO 01-23 17:13:51 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 01-23 17:13:51 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m INFO 01-23 17:13:51 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m INFO 01-23 17:13:51 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46510a003134b41af2b80477b92585e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 17:14:02 model_runner.py:1067] Loading model weights took 32.8892 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m INFO 01-23 17:14:02 model_runner.py:1067] Loading model weights took 32.8892 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m INFO 01-23 17:14:02 model_runner.py:1067] Loading model weights took 32.8892 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m INFO 01-23 17:14:02 model_runner.py:1067] Loading model weights took 32.8892 GB\n",
      "INFO 01-23 17:14:06 distributed_gpu_executor.py:57] # GPU blocks: 77774, # CPU blocks: 3276\n",
      "INFO 01-23 17:14:06 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 151.90x\n",
      "INFO 01-23 17:14:08 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-23 17:14:08 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m INFO 01-23 17:14:08 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m INFO 01-23 17:14:08 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-23 17:14:08 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m INFO 01-23 17:14:08 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m INFO 01-23 17:14:08 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m INFO 01-23 17:14:08 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 01-23 17:14:49 custom_all_reduce.py:233] Registering 10626 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m INFO 01-23 17:14:49 custom_all_reduce.py:233] Registering 10626 cuda graph addresses\n",
      "INFO 01-23 17:14:49 custom_all_reduce.py:233] Registering 10626 cuda graph addresses\n",
      "INFO 01-23 17:14:49 custom_all_reduce.py:233] Registering 10626 cuda graph addresses\n",
      "INFO 01-23 17:14:49 model_runner.py:1523] Graph capturing finished in 41 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196765)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196763)\u001b[0;0m INFO 01-23 17:14:49 model_runner.py:1523] Graph capturing finished in 41 secs.\n",
      "INFO 01-23 17:14:49 model_runner.py:1523] Graph capturing finished in 40 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196764)\u001b[0;0m INFO 01-23 17:14:49 model_runner.py:1523] Graph capturing finished in 41 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=8192, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.98, max_num_seqs=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee9aaf8-5f8e-43a1-80a3-997a32771a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doubt(BaseModel):\n",
    "    doubt: str\n",
    "\n",
    "ta = TypeAdapter(list[Doubt])\n",
    "\n",
    "json_schema = ta.json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ee8f13-1a8c-4726-a7b3-0c0d2f3ab847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18369"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"chengxuphd/liar2\")['train']\n",
    "ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1fadc0-4c36-476c-aaac-bcf5baf819d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18239"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.filter(lambda row: None not in (row['speaker_description'], row['context'], row['date'], row['statement']))\n",
    "ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "478c8c2c-485a-44ba-8bda-8c7e128b3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doubts(speakers, contexts, dates, statements):\n",
    "    messages = [[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"Speaker:\\n\" + speaker  + \"\\n\\nContext:\\n\" + context + \"\\n\\nDate:\\n\" + date + \"\\n\\nStatement:\\n\" + statement}]\n",
    "                for speaker, context, date, statement in zip(speakers, contexts, dates, statements)]\n",
    "\n",
    "    guided_decoding_params = GuidedDecodingParams(json=json_schema)\n",
    "    outputs = llm.chat(messages, SamplingParams(temperature=0.3, top_p=0.9, max_tokens=1024, guided_decoding=guided_decoding_params))\n",
    "\n",
    "    doubts = []\n",
    "    for output in outputs:\n",
    "        response = output.outputs[0].text.strip()\n",
    "        doubt = []\n",
    "        try:\n",
    "            doubt = json.loads(response)\n",
    "        except Exception:\n",
    "            pass\n",
    "            #print(traceback.format_exc())\n",
    "\n",
    "        doubts.append(doubt)\n",
    "        \n",
    "    return {\n",
    "        \"critical_doubts\": doubts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00e927fc-a607-4b15-818a-9510cfe59aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23815f45edb7403bac96913592e2229b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessed prompts:   0% 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "\u001b[Acessed prompts:   3% 1/32 [00:01<00:38,  1.23s/it, est. speed input: 368.31 toks/s, output: 4.07 toks/s]\n",
      "\u001b[Acessed prompts:  94% 30/32 [00:07<00:00,  4.08it/s, est. speed input: 1928.89 toks/s, output: 46.95 toks/s]\n",
      "\u001b[Acessed prompts:  97% 31/32 [00:08<00:00,  3.92it/s, est. speed input: 1871.70 toks/s, output: 72.95 toks/s]\n",
      "Processed prompts: 100% 32/32 [00:08<00:00,  3.80it/s, est. speed input: 1863.43 toks/s, output: 100.21 toks/s]\n"
     ]
    }
   ],
   "source": [
    "doubts_ds = ds.select(range(32)).map(create_doubts, input_columns=['speaker_description', 'context', 'date', 'statement'], batched=True, batch_size=ds.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b89663f-6165-47df-b5fc-e9e50e629192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 13847,\n",
       " 'label': 5,\n",
       " 'statement': '90 percent of Americans \"support universal background checks\" for gun purchases.',\n",
       " 'date': 'October 2, 2017',\n",
       " 'subject': 'government regulation;polls and public opinion;guns',\n",
       " 'speaker': 'chris abele',\n",
       " 'speaker_description': 'Chris Abele is Milwaukee County Executive, a position he won in an April 2011 special election to finish out the final year of the term of Scott Walker, who was elected governor in November 2010. The election was the first attempt at political office for Abele, a Milwaukee philanthropist and business owner.\\r\\nThe office is nonpartisan, but Abele has indicated he is a Democrat.',\n",
       " 'state_info': 'wisconsin',\n",
       " 'true_counts': 1,\n",
       " 'mostly_true_counts': 4,\n",
       " 'half_true_counts': 5,\n",
       " 'mostly_false_counts': 3,\n",
       " 'false_counts': 5,\n",
       " 'pants_on_fire_counts': 2,\n",
       " 'context': 'a tweet',\n",
       " 'justification': '\"Universal\" is the term for background checks to be done on every gun sale. We found support for that policy at 94 percent in the latest national poll. Support ranged between 84 percent and 89 percent in the four other most recent polls. Experts say support at or near 90 percent has been consistent for years. For a statement that is accurate and has nothing significant missing, our rating is .',\n",
       " 'critical_doubts': [{'doubt': \"The term 'universal background checks' might be misleading, as it's unclear what specific aspects of the checks are being referred to, such as private sales or mental health evaluations.\"},\n",
       "  {'doubt': 'The statistic may be based on outdated data, and public opinion could have shifted significantly since the last survey.'},\n",
       "  {'doubt': 'The phrasing of the question in the survey could have influenced the response, leading to an inaccurate representation of true public opinion.'},\n",
       "  {'doubt': 'The 90 percent figure might be an aggregate of multiple polls with varying methodologies, which could skew the results.'},\n",
       "  {'doubt': \"It's possible that the survey only polled a specific demographic, rather than a representative sample of the entire American population.\"},\n",
       "  {'doubt': \"The statement doesn't account for the nuances of gun ownership laws, which can differ significantly from state to state.\"},\n",
       "  {'doubt': \"The tweet's brevity lacks context about what constitutes a 'universal background check', which could lead to misinterpretation.\"},\n",
       "  {'doubt': 'The source of the statistic is not provided, raising questions about the credibility and potential bias of the data.'}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubts_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed913b21-a140-4805-b8d1-3e3a8e569361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87b30919d9642b3afc6e81a6fbe9a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b537ba057c4541b4e5fc6e18c839b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2243c85a69454990b91eeb4beb99e143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/amang1802/liar2-doubts/commit/e3e0b881b715c00ebad4952fdcbbd537ea87ffef', commit_message='Upload dataset', commit_description='', oid='e3e0b881b715c00ebad4952fdcbbd537ea87ffef', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/amang1802/liar2-doubts', endpoint='https://huggingface.co', repo_type='dataset', repo_id='amang1802/liar2-doubts'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubts_ds.push_to_hub('amang1802/liar2-doubts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d6713-ab96-4bac-9601-47f6e5102c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
