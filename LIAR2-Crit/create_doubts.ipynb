{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86db6785-2d73-471e-b709-6450b174d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2add563c-7125-43ed-a566-90a1fc2ba873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from pydantic import BaseModel, TypeAdapter\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "import json\n",
    "import random\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e016f04-5a40-4355-825c-a8f7870069f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = 1\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8c3a1b-b200-49f0-85d2-888ed994988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "# Instructions\n",
    "\n",
    "Simon loves Socrates and never takes any statement at face value. Unfortunately, that sometimes mean that nothing convinces him of the truth of any statement. \n",
    "\n",
    "- Your goal is to impersonate Simon.\n",
    "- Assume all statements are false and made with a malicious intent to spread misinformation.\n",
    "- Output as many as possible reasons why he might question its truth.\n",
    "- These explanations are intelligent and rational.\n",
    "- They are intended to motivate research to answer the truth.\n",
    "- Keep these doubts short.\n",
    "- Make sure any two doubts are different from each other. We don't want similar doubts appearing multiple times.\n",
    "\n",
    "# Output Instructions\n",
    "\n",
    "Reply in JSON using the following format.\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"doubt\": \"string\"\n",
    "  },\n",
    "  {\n",
    "    \"doubt\": \"string\"\n",
    "  },\n",
    "  {\n",
    "    \"doubt\": \"string\"\n",
    "  },\n",
    "...\n",
    "]\n",
    "\n",
    "# Example:\n",
    "\n",
    "Speaker:\n",
    "Kathleen Sullivan is the town of Warren’s substance abuse coordinator. She is also program director for The BAY Team, Barrington’s Prevention Coalition.\n",
    "\n",
    "Context:\n",
    "testimony before the House Judiciary Committee\n",
    "\n",
    "Date:\n",
    "April 29, 2015\n",
    "\n",
    "Statement:\n",
    "The proportion of Rhode Islanders entering substance abuse treatment primarily due to marijuana use has reached its highest point in 20 years.\n",
    "\n",
    "Response:\n",
    "[\n",
    "  {\n",
    "    \"doubt\": \"Marijuana is legal now, and there are lots of studies which show that actually reduces substance abuse.\"\n",
    "  },\n",
    "  {\n",
    "    \"doubt\": \"While some drugs and opioids continue to cause a substance abuse problem, Marijuana substance has likely already peaked.\"\n",
    "  },\n",
    "  {\n",
    "    \"doubt\": \"It's a play on words and the truth maybe that it peaked sometime in the last 20 years.\"\n",
    "  }\n",
    "]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d4947d-52f8-43ae-a920-5d4e0373690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177c74e1-107b-4356-9485-8f5a928492e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-16 08:25:37 config.py:510] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 01-16 08:25:37 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
      "INFO 01-16 08:25:38 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-16 08:25:40 model_runner.py:1094] Starting to load model neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8...\n",
      "INFO 01-16 08:25:40 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd4f17a27ce4be1a8a90f5f8cab3b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-16 08:25:50 model_runner.py:1099] Loading model weights took 67.6981 GB\n",
      "INFO 01-16 08:25:52 worker.py:241] Memory profiling takes 1.49 seconds\n",
      "INFO 01-16 08:25:52 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 01-16 08:25:52 worker.py:241] model weights take 67.70GiB; non_torch_memory takes 0.25GiB; PyTorch activation peak memory takes 1.69GiB; the rest of the memory reserved for KV Cache is 67.29GiB.\n",
      "INFO 01-16 08:25:52 gpu_executor.py:76] # GPU blocks: 13780, # CPU blocks: 819\n",
      "INFO 01-16 08:25:52 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 26.91x\n",
      "INFO 01-16 08:25:54 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100% 19/19 [00:08<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-16 08:26:03 model_runner.py:1535] Graph capturing finished in 9 secs, took 0.34 GiB\n",
      "INFO 01-16 08:26:03 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 12.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=8192, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.98, max_num_seqs=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee9aaf8-5f8e-43a1-80a3-997a32771a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doubt(BaseModel):\n",
    "    doubt: str\n",
    "\n",
    "ta = TypeAdapter(list[Doubt])\n",
    "\n",
    "json_schema = ta.json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ee8f13-1a8c-4726-a7b3-0c0d2f3ab847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18369"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"chengxuphd/liar2\")['train']\n",
    "ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1fadc0-4c36-476c-aaac-bcf5baf819d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18239"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.filter(lambda row: None not in (row['speaker_description'], row['context'], row['date'], row['statement']))\n",
    "ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "478c8c2c-485a-44ba-8bda-8c7e128b3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doubts(speakers, contexts, dates, statements):\n",
    "    messages = [[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"Speaker:\\n\" + speaker  + \"\\n\\nContext:\\n\" + context + \"\\n\\nDate:\\n\" + date + \"\\n\\nStatement:\\n\" + statement}]\n",
    "                for speaker, context, date, statement in zip(speakers, contexts, dates, statements)]\n",
    "\n",
    "    guided_decoding_params = GuidedDecodingParams(json=json_schema)\n",
    "    outputs = llm.chat(messages, SamplingParams(temperature=0.3, top_p=0.9, max_tokens=1024, guided_decoding=guided_decoding_params))\n",
    "\n",
    "    doubts = \"\"\n",
    "    for output in outputs:\n",
    "        response = output.outputs[0].text.strip()\n",
    "        doubt = []\n",
    "        try:\n",
    "            doubt = json.loads(response)\n",
    "        except Exception:\n",
    "            pass\n",
    "            #print(traceback.format_exc())\n",
    "\n",
    "        doubts.append(doubt)\n",
    "        \n",
    "    return {\n",
    "        \"critical_doubts\": doubts,\n",
    "    }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e927fc-a607-4b15-818a-9510cfe59aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function create_doubts at 0x7bfc8ef9d630> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6805ab777cd4dc7a224a9b3e9c146fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-16 08:26:04 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessed prompts:   0% 0/18239 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "\u001b[Acessed prompts:   0% 1/18239 [00:20<102:14:41, 20.18s/it, est. speed input: 23.98 toks/s, output: 9.36 toks/s]\n",
      "\u001b[Acessed prompts:   0% 2/18239 [00:20<42:28:04,  8.38s/it, est. speed input: 48.41 toks/s, output: 18.66 toks/s]\n",
      "\u001b[Acessed prompts:   0% 3/18239 [00:20<23:44:56,  4.69s/it, est. speed input: 70.83 toks/s, output: 27.72 toks/s]\n",
      "\u001b[Acessed prompts:   0% 5/18239 [00:20<10:48:27,  2.13s/it, est. speed input: 115.18 toks/s, output: 45.68 toks/s]\n",
      "\u001b[Acessed prompts:   0% 6/18239 [00:21<8:12:19,  1.62s/it, est. speed input: 135.92 toks/s, output: 54.23 toks/s] \n",
      "\u001b[Acessed prompts:   0% 8/18239 [00:21<4:51:24,  1.04it/s, est. speed input: 175.24 toks/s, output: 71.91 toks/s]\n",
      "\u001b[Acessed prompts:   0% 9/18239 [00:21<3:54:45,  1.29it/s, est. speed input: 194.92 toks/s, output: 80.58 toks/s]\n",
      "\u001b[Acessed prompts:   0% 10/18239 [00:21<3:09:10,  1.61it/s, est. speed input: 216.76 toks/s, output: 89.20 toks/s]\n",
      "\u001b[Acessed prompts:   0% 12/18239 [00:22<2:10:08,  2.33it/s, est. speed input: 261.39 toks/s, output: 106.42 toks/s]\n",
      "\u001b[Acessed prompts:   0% 13/18239 [00:22<1:53:13,  2.68it/s, est. speed input: 283.67 toks/s, output: 114.85 toks/s]\n",
      "\u001b[Acessed prompts:   0% 16/18239 [00:22<1:14:31,  4.08it/s, est. speed input: 342.74 toks/s, output: 140.72 toks/s]\n",
      "\u001b[Acessed prompts:   0% 19/18239 [00:23<57:26,  5.29it/s, est. speed input: 399.92 toks/s, output: 166.29 toks/s]  \n",
      "\u001b[Acessed prompts:   0% 22/18239 [00:23<43:59,  6.90it/s, est. speed input: 456.36 toks/s, output: 192.26 toks/s]\n",
      "\u001b[Acessed prompts:   0% 24/18239 [00:23<42:41,  7.11it/s, est. speed input: 491.41 toks/s, output: 208.38 toks/s]\n",
      "\u001b[Acessed prompts:   0% 26/18239 [00:23<38:30,  7.88it/s, est. speed input: 524.89 toks/s, output: 224.96 toks/s]\n",
      "\u001b[Acessed prompts:   0% 31/18239 [00:24<35:02,  8.66it/s, est. speed input: 610.12 toks/s, output: 264.78 toks/s]\n",
      "\u001b[Acessed prompts:   0% 39/18239 [00:24<26:32, 11.43it/s, est. speed input: 750.87 toks/s, output: 329.93 toks/s]\n",
      "\u001b[Acessed prompts:   0% 41/18239 [00:25<26:59, 11.24it/s, est. speed input: 785.71 toks/s, output: 345.02 toks/s]\n",
      "\u001b[Acessed prompts:   0% 46/18239 [00:25<26:24, 11.48it/s, est. speed input: 866.17 toks/s, output: 382.75 toks/s]\n",
      "\u001b[Acessed prompts:   0% 51/18239 [00:25<27:33, 11.00it/s, est. speed input: 950.76 toks/s, output: 418.28 toks/s]\n",
      "\u001b[Acessed prompts:   0% 54/18239 [00:26<26:47, 11.31it/s, est. speed input: 994.43 toks/s, output: 440.03 toks/s]\n",
      "\u001b[Acessed prompts:   0% 57/18239 [00:26<28:08, 10.77it/s, est. speed input: 1038.56 toks/s, output: 460.09 toks/s]\n",
      "\u001b[Acessed prompts:   0% 60/18239 [00:26<31:03,  9.76it/s, est. speed input: 1077.54 toks/s, output: 478.67 toks/s]\n",
      "\u001b[Acessed prompts:   0% 62/18239 [00:27<30:13, 10.03it/s, est. speed input: 1105.77 toks/s, output: 492.41 toks/s]\n",
      "\u001b[Acessed prompts:   0% 64/18239 [00:27<29:50, 10.15it/s, est. speed input: 1130.25 toks/s, output: 505.87 toks/s]\n",
      "\u001b[Acessed prompts:   0% 66/18239 [00:27<30:06, 10.06it/s, est. speed input: 1158.43 toks/s, output: 518.87 toks/s]\n",
      "\u001b[Acessed prompts:   0% 68/18239 [00:27<33:39,  9.00it/s, est. speed input: 1184.89 toks/s, output: 530.11 toks/s]\n",
      "\u001b[Acessed prompts:   0% 73/18239 [00:28<27:14, 11.12it/s, est. speed input: 1259.46 toks/s, output: 565.11 toks/s]\n",
      "\u001b[Acessed prompts:   0% 77/18239 [00:28<29:40, 10.20it/s, est. speed input: 1312.37 toks/s, output: 588.87 toks/s]\n",
      "\u001b[Acessed prompts:   0% 81/18239 [00:28<27:38, 10.95it/s, est. speed input: 1366.44 toks/s, output: 615.09 toks/s]\n",
      "\u001b[Acessed prompts:   0% 83/18239 [00:29<29:54, 10.12it/s, est. speed input: 1384.94 toks/s, output: 625.89 toks/s]\n",
      "\u001b[Acessed prompts:   0% 85/18239 [00:29<31:36,  9.57it/s, est. speed input: 1404.94 toks/s, output: 636.80 toks/s]\n",
      "\u001b[Acessed prompts:   0% 87/18239 [00:29<29:16, 10.34it/s, est. speed input: 1429.21 toks/s, output: 649.93 toks/s]\n",
      "\u001b[Acessed prompts:   0% 89/18239 [00:29<29:02, 10.42it/s, est. speed input: 1455.21 toks/s, output: 662.05 toks/s]\n",
      "\u001b[Acessed prompts:   0% 91/18239 [00:29<26:59, 11.21it/s, est. speed input: 1481.97 toks/s, output: 675.13 toks/s]\n",
      "\u001b[Acessed prompts:   1% 94/18239 [00:30<31:10,  9.70it/s, est. speed input: 1515.76 toks/s, output: 690.63 toks/s]\n",
      "\u001b[Acessed prompts:   1% 97/18239 [00:30<31:56,  9.47it/s, est. speed input: 1549.23 toks/s, output: 707.05 toks/s]\n",
      "\u001b[Acessed prompts:   1% 100/18239 [00:30<32:05,  9.42it/s, est. speed input: 1580.36 toks/s, output: 723.61 toks/s]\n",
      "\u001b[Acessed prompts:   1% 102/18239 [00:31<29:35, 10.21it/s, est. speed input: 1605.00 toks/s, output: 736.22 toks/s]\n",
      "\u001b[Acessed prompts:   1% 104/18239 [00:31<31:31,  9.59it/s, est. speed input: 1626.40 toks/s, output: 746.40 toks/s]\n",
      "\u001b[Acessed prompts:   1% 107/18239 [00:31<29:46, 10.15it/s, est. speed input: 1657.37 toks/s, output: 763.97 toks/s]\n",
      "\u001b[Acessed prompts:   1% 109/18239 [00:31<31:55,  9.46it/s, est. speed input: 1675.59 toks/s, output: 773.68 toks/s]\n",
      "\u001b[Acessed prompts:   1% 112/18239 [00:32<36:08,  8.36it/s, est. speed input: 1698.60 toks/s, output: 786.83 toks/s]\n",
      "\u001b[Acessed prompts:   1% 113/18239 [00:32<36:13,  8.34it/s, est. speed input: 1709.64 toks/s, output: 791.82 toks/s]\n",
      "\u001b[Acessed prompts:   1% 114/18239 [00:32<39:33,  7.64it/s, est. speed input: 1714.51 toks/s, output: 795.24 toks/s]\n",
      "\u001b[Acessed prompts:   1% 116/18239 [00:32<33:37,  8.98it/s, est. speed input: 1737.56 toks/s, output: 807.61 toks/s]\n",
      "\u001b[Acessed prompts:   1% 117/18239 [00:33<47:43,  6.33it/s, est. speed input: 1734.93 toks/s, output: 806.81 toks/s]\n",
      "\u001b[Acessed prompts:   1% 119/18239 [00:33<45:11,  6.68it/s, est. speed input: 1752.33 toks/s, output: 816.16 toks/s]\n",
      "\u001b[Acessed prompts:   1% 120/18239 [00:33<47:32,  6.35it/s, est. speed input: 1758.86 toks/s, output: 819.54 toks/s]\n",
      "\u001b[Acessed prompts:   1% 121/18239 [00:33<44:46,  6.74it/s, est. speed input: 1767.82 toks/s, output: 824.68 toks/s]\n",
      "\u001b[Acessed prompts:   1% 123/18239 [00:33<45:30,  6.64it/s, est. speed input: 1782.93 toks/s, output: 833.02 toks/s]\n",
      "\u001b[Acessed prompts:   1% 126/18239 [00:34<48:35,  6.21it/s, est. speed input: 1799.24 toks/s, output: 844.11 toks/s]\n",
      "\u001b[Acessed prompts:   1% 127/18239 [00:35<1:11:29,  4.22it/s, est. speed input: 1786.12 toks/s, output: 838.06 toks/s]\n",
      "\u001b[Acessed prompts:   1% 128/18239 [00:35<1:45:44,  2.85it/s, est. speed input: 1761.04 toks/s, output: 827.68 toks/s]\n",
      "\u001b[Acessed prompts:   1% 129/18239 [00:41<7:58:21,  1.58s/it, est. speed input: 1524.83 toks/s, output: 716.19 toks/s]\n",
      "\u001b[Acessed prompts:   1% 130/18239 [00:42<6:54:19,  1.37s/it, est. speed input: 1512.02 toks/s, output: 708.70 toks/s]\n",
      "\u001b[Acessed prompts:   1% 133/18239 [00:42<3:43:06,  1.35it/s, est. speed input: 1529.59 toks/s, output: 715.92 toks/s]\n",
      "\u001b[Acessed prompts:   1% 134/18239 [00:42<3:10:34,  1.58it/s, est. speed input: 1533.42 toks/s, output: 717.52 toks/s]\n",
      "\u001b[Acessed prompts:   1% 137/18239 [00:43<2:05:10,  2.41it/s, est. speed input: 1547.95 toks/s, output: 723.54 toks/s]\n",
      "\u001b[Acessed prompts:   1% 139/18239 [00:43<1:39:15,  3.04it/s, est. speed input: 1558.98 toks/s, output: 728.90 toks/s]\n",
      "\u001b[Acessed prompts:   1% 141/18239 [00:44<1:23:57,  3.59it/s, est. speed input: 1567.99 toks/s, output: 732.70 toks/s]\n",
      "\u001b[Acessed prompts:   1% 142/18239 [00:44<1:19:18,  3.80it/s, est. speed input: 1573.71 toks/s, output: 734.84 toks/s]\n",
      "\u001b[Acessed prompts:   1% 146/18239 [00:44<49:08,  6.14it/s, est. speed input: 1608.30 toks/s, output: 749.83 toks/s]  \n",
      "\u001b[Acessed prompts:   1% 151/18239 [00:44<39:04,  7.72it/s, est. speed input: 1644.68 toks/s, output: 764.62 toks/s]\n",
      "\u001b[Acessed prompts:   1% 152/18239 [00:45<41:21,  7.29it/s, est. speed input: 1647.66 toks/s, output: 766.43 toks/s]\n",
      "\u001b[Acessed prompts:   1% 153/18239 [00:45<43:36,  6.91it/s, est. speed input: 1652.86 toks/s, output: 767.97 toks/s]\n",
      "\u001b[Acessed prompts:   1% 154/18239 [00:45<42:27,  7.10it/s, est. speed input: 1659.47 toks/s, output: 770.80 toks/s]\n",
      "\u001b[Acessed prompts:   1% 156/18239 [00:45<46:02,  6.55it/s, est. speed input: 1670.75 toks/s, output: 774.02 toks/s]\n",
      "\u001b[Acessed prompts:   1% 157/18239 [00:46<50:54,  5.92it/s, est. speed input: 1672.33 toks/s, output: 774.87 toks/s]\n",
      "\u001b[Acessed prompts:   1% 161/18239 [00:46<34:32,  8.72it/s, est. speed input: 1709.02 toks/s, output: 788.69 toks/s]\n",
      "\u001b[Acessed prompts:   1% 162/18239 [00:46<38:08,  7.90it/s, est. speed input: 1715.39 toks/s, output: 790.34 toks/s]\n",
      "\u001b[Acessed prompts:   1% 163/18239 [00:46<46:05,  6.54it/s, est. speed input: 1715.34 toks/s, output: 790.47 toks/s]\n",
      "\u001b[Acessed prompts:   1% 164/18239 [00:46<44:10,  6.82it/s, est. speed input: 1721.08 toks/s, output: 792.85 toks/s]\n",
      "\u001b[Acessed prompts:   1% 166/18239 [00:47<41:38,  7.23it/s, est. speed input: 1733.17 toks/s, output: 797.75 toks/s]\n",
      "\u001b[Acessed prompts:   1% 167/18239 [00:47<40:29,  7.44it/s, est. speed input: 1738.54 toks/s, output: 800.17 toks/s]\n",
      "\u001b[Acessed prompts:   1% 169/18239 [00:47<39:25,  7.64it/s, est. speed input: 1748.49 toks/s, output: 805.01 toks/s]\n",
      "\u001b[Acessed prompts:   1% 172/18239 [00:47<30:50,  9.77it/s, est. speed input: 1769.96 toks/s, output: 815.33 toks/s]\n",
      "\u001b[Acessed prompts:   1% 175/18239 [00:48<31:57,  9.42it/s, est. speed input: 1789.83 toks/s, output: 823.81 toks/s]\n",
      "\u001b[Acessed prompts:   1% 178/18239 [00:48<27:57, 10.77it/s, est. speed input: 1815.08 toks/s, output: 832.85 toks/s]\n",
      "\u001b[Acessed prompts:   1% 180/18239 [00:48<32:28,  9.27it/s, est. speed input: 1824.64 toks/s, output: 837.01 toks/s]\n",
      "\u001b[Acessed prompts:   1% 181/18239 [00:48<36:16,  8.30it/s, est. speed input: 1828.83 toks/s, output: 839.08 toks/s]\n",
      "\u001b[Acessed prompts:   1% 184/18239 [00:48<29:53, 10.07it/s, est. speed input: 1855.56 toks/s, output: 849.11 toks/s]\n",
      "\u001b[Acessed prompts:   1% 186/18239 [00:49<36:04,  8.34it/s, est. speed input: 1861.91 toks/s, output: 852.32 toks/s]\n",
      "\u001b[Acessed prompts:   1% 190/18239 [00:49<28:56, 10.39it/s, est. speed input: 1889.52 toks/s, output: 866.59 toks/s]\n",
      "\u001b[Acessed prompts:   1% 192/18239 [00:49<33:17,  9.04it/s, est. speed input: 1898.56 toks/s, output: 870.26 toks/s]\n",
      "\u001b[Acessed prompts:   1% 194/18239 [00:50<31:46,  9.47it/s, est. speed input: 1911.43 toks/s, output: 875.72 toks/s]\n",
      "\u001b[Acessed prompts:   1% 196/18239 [00:50<33:18,  9.03it/s, est. speed input: 1919.93 toks/s, output: 880.03 toks/s]\n",
      "\u001b[Acessed prompts:   1% 197/18239 [00:50<34:15,  8.78it/s, est. speed input: 1923.62 toks/s, output: 881.94 toks/s]\n",
      "\u001b[Acessed prompts:   1% 198/18239 [00:50<44:57,  6.69it/s, est. speed input: 1921.63 toks/s, output: 881.82 toks/s]\n",
      "\u001b[Acessed prompts:   1% 201/18239 [00:50<34:40,  8.67it/s, est. speed input: 1944.09 toks/s, output: 892.17 toks/s]\n",
      "\u001b[Acessed prompts:   1% 202/18239 [00:51<38:28,  7.81it/s, est. speed input: 1946.11 toks/s, output: 893.05 toks/s]\n",
      "\u001b[Acessed prompts:   1% 205/18239 [00:51<38:41,  7.77it/s, est. speed input: 1959.58 toks/s, output: 899.94 toks/s]\n",
      "\u001b[Acessed prompts:   1% 207/18239 [00:51<40:18,  7.45it/s, est. speed input: 1967.44 toks/s, output: 903.88 toks/s]\n",
      "\u001b[Acessed prompts:   1% 210/18239 [00:52<32:58,  9.11it/s, est. speed input: 1988.15 toks/s, output: 914.31 toks/s]\n",
      "\u001b[Acessed prompts:   1% 213/18239 [00:52<33:08,  9.07it/s, est. speed input: 2001.40 toks/s, output: 921.34 toks/s]\n",
      "\u001b[Acessed prompts:   1% 214/18239 [00:52<39:24,  7.62it/s, est. speed input: 2001.34 toks/s, output: 921.66 toks/s]\n",
      "\u001b[Acessed prompts:   1% 215/18239 [00:53<53:42,  5.59it/s, est. speed input: 1994.31 toks/s, output: 919.39 toks/s]\n",
      "\u001b[Acessed prompts:   1% 217/18239 [00:53<51:29,  5.83it/s, est. speed input: 2002.30 toks/s, output: 923.03 toks/s]\n",
      "\u001b[Acessed prompts:   1% 219/18239 [00:53<41:54,  7.17it/s, est. speed input: 2013.63 toks/s, output: 929.41 toks/s]"
     ]
    }
   ],
   "source": [
    "doubts_ds = ds.map(create_doubts, input_columns=['speaker_description', 'context', 'date', 'statement'], batched=True, batch_size=ds.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed913b21-a140-4805-b8d1-3e3a8e569361",
   "metadata": {},
   "outputs": [],
   "source": [
    "doubts_ds.push_to_hub('amang1802/liar2-doubts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d6713-ab96-4bac-9601-47f6e5102c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
