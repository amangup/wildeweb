{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcf6e57-1c59-44c3-8710-a3fa927b4046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 01:45:06 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from jinja2 import Template\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0107fb4c-5a46-4dbb-8774-209b71168555",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = 4\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fdeca6-d6b4-4d2f-a438-6394da58945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('amang1802/wildeweb_cls_1M')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7950f9-8ff3-40f4-b059-da2796d0a2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'dump', 'url', 'file_path', 'language', 'language_score', 'token_count', 'score', 'int_score', 'justification', 'classification_score'],\n",
       "    num_rows: 1000000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8196360-e59b-4ce2-bbcd-c012f0ae1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Below is an extract from a web page. You are an AI content evaluator focused on assessing educational material's value for soft skills development. Soft skills include conversational ability, empathy, leadership skills, public speaking, confidence building, critical thinking, problem solving, professional writing, teamwork, digital literacy, professional attitude, work ethic, career management and intercultural fluency. \n",
    "\n",
    "You will analyze content using the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion:\n",
    "- Add 1 point if the extract hows superficial coverage of basic communication and teamwork concepts without meaningful depth or practical application. Professional development opportunities are limited to theoretical knowledge, and problem-solving scenarios lack complexity or real-world context. Cultural awareness and digital literacy elements are either absent or extremely basic.\n",
    "- Add another point if the extract specifically includes discussion of soft skills and includes straightforward communication scenarios and simple team dynamics, but lacks nuanced interaction or complex problem-solving opportunities. Professional development focuses on fundamental skills with limited practical application, while cultural awareness and digital literacy are present but superficial.\n",
    "- Award a third point if the extract specifically includes discussion of soft skills andfeatures realistic scenarios that integrate emotional intelligence, leadership challenges, and critical thinking opportunities. Professional development includes practical applications with meaningful context, while incorporating cultural awareness and modern digital literacy skills throughout the material. \n",
    "- Grant a fourth point if the extract specifically includes discussion of soft skills and presents complex scenarios requiring sophisticated communication, strategic thinking, and advanced problem-solving across multiple contexts. Professional development opportunities are comprehensive and practical, with strong emphasis on intercultural fluency and technological adaptation.\n",
    "- Bestow a fifth point if the extract specifically includes discussion of soft skills and seamlessly integrates advanced communication, leadership, and problem-solving scenarios that mirror real-world complexity. Professional development opportunities span multiple contexts with sophisticated cultural awareness, while digital literacy and practical application are woven throughout every element.\n",
    "\n",
    "After examining the extract: \n",
    "- Briefly justify your total score, up to 100 words.\n",
    "- Conclude with the score using the format: \"Educational score: <total points>\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5078a1c4-c45f-4ff8-9c80-a0e6b96a5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-27b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a56b4b-5c8c-4f2f-ac1f-9c68337d7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 01:45:07 [config.py:2537] For Gemma 2 and 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.\n",
      "INFO 03-19 01:45:07 [config.py:2595] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 03-19 01:45:14 [config.py:583] This model supports multiple tasks: {'embed', 'reward', 'generate', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 03-19 01:45:14 [arg_utils.py:1754] ['Gemma3ForConditionalGeneration'] is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 03-19 01:45:14 [config.py:1515] Defaulting to use mp for distributed inference\n",
      "INFO 03-19 01:45:14 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.0) with config: model='google/gemma-3-27b-it', speculative_config=None, tokenizer='google/gemma-3-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=google/gemma-3-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 03-19 01:45:16 [multiproc_worker_utils.py:310] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-19 01:45:16 [custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m INFO 03-19 01:45:16 [multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:16 [multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:16 [multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 03-19 01:45:19 [cuda.py:285] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:19 [cuda.py:285] Using Flash Attention backend.\n",
      "INFO 03-19 01:45:19 [cuda.py:285] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:19 [cuda.py:285] Using Flash Attention backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W319 01:45:20.527957295 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W319 01:45:20.528419069 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W319 01:45:20.705683585 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W319 01:45:20.706057837 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W319 01:45:20.726162545 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W319 01:45:20.726377189 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 01:45:20 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "INFO 03-19 01:45:20 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:20 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "INFO 03-19 01:45:20 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "INFO 03-19 01:45:20 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m INFO 03-19 01:45:20 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:20 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 03-19 01:45:20 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W319 01:45:20.732822939 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W319 01:45:20.733028929 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 01:45:22 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:22 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 03-19 01:45:22 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 03-19 01:45:22 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 03-19 01:45:22 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ea89b86a'), local_subscribe_addr='ipc:///tmp/c1bdbbc4-ff0b-48ee-af51-06501445a066', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 03-19 01:45:22 [parallel_state.py:967] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:22 [parallel_state.py:967] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\n",
      "INFO 03-19 01:45:22 [parallel_state.py:967] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 03-19 01:45:22 [parallel_state.py:967] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n",
      "INFO 03-19 01:45:22 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:22 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\n",
      "INFO 03-19 01:45:22 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\n",
      "INFO 03-19 01:45:22 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:22 [cuda.py:269] Cannot use FlashAttention-2 backend for head size 72.\n",
      "INFO 03-19 01:45:22 [cuda.py:269] Cannot use FlashAttention-2 backend for head size 72.\n",
      "INFO 03-19 01:45:22 [cuda.py:282] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:22 [cuda.py:282] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m INFO 03-19 01:45:22 [cuda.py:269] Cannot use FlashAttention-2 backend for head size 72.\n",
      "INFO 03-19 01:45:22 [cuda.py:269] Cannot use FlashAttention-2 backend for head size 72.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:22 [cuda.py:282] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m INFO 03-19 01:45:22 [cuda.py:282] Using XFormers backend.\n",
      "INFO 03-19 01:45:22 [config.py:3222] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:22 [config.py:3222] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n",
      "INFO 03-19 01:45:22 [config.py:3222] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:22 [config.py:3222] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:23 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
      "INFO 03-19 01:45:23 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m INFO 03-19 01:45:23 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
      "INFO 03-19 01:45:23 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ff2884bccc4a9aa0ce06bf9499e205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m INFO 03-19 01:45:27 [loader.py:429] Loading weights took 4.45 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:27 [loader.py:429] Loading weights took 4.56 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:27 [loader.py:429] Loading weights took 4.48 seconds\n",
      "INFO 03-19 01:45:27 [loader.py:429] Loading weights took 4.44 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m INFO 03-19 01:45:28 [model_runner.py:1146] Model loading took 13.1666 GB and 5.617136 seconds\n",
      "INFO 03-19 01:45:28 [model_runner.py:1146] Model loading took 13.1666 GB and 5.430336 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:28 [model_runner.py:1146] Model loading took 13.1666 GB and 5.458013 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:28 [model_runner.py:1146] Model loading took 13.1666 GB and 5.573651 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:33 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m WARNING 03-19 01:45:34 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 260) is too short to hold the multi-modal embeddings in the worst case (261 tokens in total, out of which {'image': 260} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:40 [worker.py:267] Memory profiling takes 12.55 seconds\n",
      "INFO 03-19 01:45:40 [worker.py:267] Memory profiling takes 12.55 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 03-19 01:45:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:40 [worker.py:267] model weights take 13.17GiB; non_torch_memory takes 4.14GiB; PyTorch activation peak memory takes 1.73GiB; the rest of the memory reserved for KV Cache is 117.88GiB.\n",
      "INFO 03-19 01:45:40 [worker.py:267] model weights take 13.17GiB; non_torch_memory takes 3.67GiB; PyTorch activation peak memory takes 1.73GiB; the rest of the memory reserved for KV Cache is 118.35GiB.\n",
      "INFO 03-19 01:45:40 [worker.py:267] Memory profiling takes 12.59 seconds\n",
      "INFO 03-19 01:45:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 03-19 01:45:40 [worker.py:267] model weights take 13.17GiB; non_torch_memory takes 4.92GiB; PyTorch activation peak memory takes 1.73GiB; the rest of the memory reserved for KV Cache is 117.10GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:41 [worker.py:267] Memory profiling takes 12.67 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:41 [worker.py:267] model weights take 13.17GiB; non_torch_memory takes 4.14GiB; PyTorch activation peak memory takes 1.73GiB; the rest of the memory reserved for KV Cache is 117.88GiB.\n",
      "INFO 03-19 01:45:41 [executor_base.py:111] # cuda blocks: 61891, # CPU blocks: 2114\n",
      "INFO 03-19 01:45:41 [executor_base.py:116] Maximum concurrency for 16384 tokens per request: 60.44x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m INFO 03-19 01:45:44 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-19 01:45:44 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0% 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:45:44 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:45:44 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  97% 34/35 [00:20<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:46:05 [custom_all_reduce.py:229] Registering 4375 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100% 35/35 [00:21<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m INFO 03-19 01:46:06 [custom_all_reduce.py:229] Registering 4375 cuda graph addresses\n",
      "INFO 03-19 01:46:06 [custom_all_reduce.py:229] Registering 4375 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:46:06 [custom_all_reduce.py:229] Registering 4375 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046560)\u001b[0;0m INFO 03-19 01:46:06 [model_runner.py:1570] Graph capturing finished in 22 secs, took 0.50 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046555)\u001b[0;0m INFO 03-19 01:46:06 [model_runner.py:1570] Graph capturing finished in 22 secs, took 0.50 GiB\n",
      "INFO 03-19 01:46:06 [model_runner.py:1570] Graph capturing finished in 22 secs, took 0.50 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1046552)\u001b[0;0m INFO 03-19 01:46:06 [model_runner.py:1570] Graph capturing finished in 22 secs, took 0.50 GiB\n",
      "INFO 03-19 01:46:06 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 38.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=16384, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bfe176e-8851-4cae-a207-b72246aaef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'Educational score: (\\d+)\\s*$'\n",
    "def extract_score(text):\n",
    "    match = re.search(pattern, text)\n",
    "    score = 0\n",
    "    if match:\n",
    "        score = int(match.group(1))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be81ccc6-6107-4b54-bf44-5749ea2d684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(texts):\n",
    "    messages = [[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"Extract:\\n\" + text[:25000] + \"\\n\\nJustification:\"}] for text in texts]\n",
    "    outputs = llm.chat(messages, SamplingParams(temperature=0, max_tokens=512))\n",
    "    responses = [output.outputs[0].text.strip() for output in outputs]\n",
    "    scores =  [extract_score(response) for response in responses]\n",
    "    return {\"justification-v2\": responses, \"classification_score_v2\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3ebd8-3b3c-40c0-9fa8-c4810f10cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function classify at 0x7ab02c0ab880> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306a6d18b88d4f2993c39d27b4b1b1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 01:46:07 [chat_utils.py:346] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessed prompts:   0% 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "\u001b[Acessed prompts:   0% 1/10000 [00:12<35:59:19, 12.96s/it, est. speed input: 75.63 toks/s, output: 6.33 toks/s]\n",
      "\u001b[Acessed prompts:   0% 2/10000 [00:13<15:16:44,  5.50s/it, est. speed input: 120.85 toks/s, output: 12.84 toks/s]\n",
      "\u001b[Acessed prompts:   0% 5/10000 [00:13<4:36:20,  1.66s/it, est. speed input: 317.21 toks/s, output: 32.10 toks/s] \n",
      "\u001b[Acessed prompts:   0% 11/10000 [00:13<1:36:17,  1.73it/s, est. speed input: 656.95 toks/s, output: 70.64 toks/s]\n",
      "\u001b[Acessed prompts:   0% 15/10000 [00:14<1:00:36,  2.75it/s, est. speed input: 855.26 toks/s, output: 96.29 toks/s]\n",
      "\u001b[Acessed prompts:   0% 20/10000 [00:14<38:56,  4.27it/s, est. speed input: 1107.62 toks/s, output: 127.28 toks/s]\n",
      "\u001b[Acessed prompts:   0% 26/10000 [00:14<25:37,  6.49it/s, est. speed input: 1414.15 toks/s, output: 164.00 toks/s]\n",
      "\u001b[Acessed prompts:   0% 31/10000 [00:14<20:20,  8.17it/s, est. speed input: 1679.67 toks/s, output: 192.90 toks/s]\n",
      "\u001b[Acessed prompts:   0% 38/10000 [00:15<15:23, 10.79it/s, est. speed input: 1977.19 toks/s, output: 233.05 toks/s]\n",
      "\u001b[Acessed prompts:   0% 41/10000 [00:15<13:47, 12.03it/s, est. speed input: 2115.60 toks/s, output: 250.19 toks/s]\n",
      "\u001b[Acessed prompts:   0% 44/10000 [00:15<12:23, 13.39it/s, est. speed input: 2239.03 toks/s, output: 267.22 toks/s]\n",
      "\u001b[Acessed prompts:   0% 47/10000 [00:15<12:49, 12.93it/s, est. speed input: 2390.95 toks/s, output: 281.88 toks/s]\n",
      "\u001b[Acessed prompts:   1% 51/10000 [00:15<12:16, 13.51it/s, est. speed input: 2616.20 toks/s, output: 302.31 toks/s]\n",
      "\u001b[Acessed prompts:   1% 58/10000 [00:16<10:52, 15.23it/s, est. speed input: 3024.45 toks/s, output: 338.52 toks/s]\n",
      "\u001b[Acessed prompts:   1% 65/10000 [00:16<09:27, 17.52it/s, est. speed input: 3414.57 toks/s, output: 375.31 toks/s]\n",
      "\u001b[Acessed prompts:   1% 72/10000 [00:17<09:47, 16.90it/s, est. speed input: 3698.32 toks/s, output: 407.89 toks/s]\n",
      "\u001b[Acessed prompts:   1% 80/10000 [00:17<09:25, 17.55it/s, est. speed input: 4043.31 toks/s, output: 445.55 toks/s]\n",
      "\u001b[Acessed prompts:   1% 85/10000 [00:17<10:03, 16.42it/s, est. speed input: 4243.49 toks/s, output: 465.90 toks/s]\n",
      "\u001b[Acessed prompts:   1% 93/10000 [00:18<09:08, 18.05it/s, est. speed input: 4633.09 toks/s, output: 503.13 toks/s]\n",
      "\u001b[Acessed prompts:   1% 102/10000 [00:18<08:20, 19.79it/s, est. speed input: 5046.29 toks/s, output: 544.66 toks/s]"
     ]
    }
   ],
   "source": [
    "total_count = 1000 * 1000\n",
    "num_steps = 100\n",
    "step_size = total_count // num_steps\n",
    "for step in range(num_steps):\n",
    "    print(f\"Running step: {step}\")\n",
    "\n",
    "    start_i = step * step_size\n",
    "    end_i = (step+1) * step_size\n",
    "    \n",
    "    # cls_ds_stream = ds.take(step_size).map(classify, batched=True, batch_size=step_size, input_columns=[\"text\"])\n",
    "    # cls_ds_list = list(cls_ds_stream)\n",
    "\n",
    "    cls_ds = ds.select(range(start_i, end_i)).map(classify, batched=True, batch_size=step_size, input_columns=[\"text\"])\n",
    "\n",
    "    #cls_ds = Dataset.from_list(cls_ds_list)\n",
    "    cls_ds.save_to_disk(f\"soft-skills-v2-cls-{step}.hf\")\n",
    "\n",
    "    #ds = ds.skip(step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f8a40-6e31-4454-8a99-65ed23e32b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
