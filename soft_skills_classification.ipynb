{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcf6e57-1c59-44c3-8710-a3fa927b4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from jinja2 import Template\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0107fb4c-5a46-4dbb-8774-209b71168555",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = 4\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fdeca6-d6b4-4d2f-a438-6394da58945d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d333a088f584c26a659c7fea3c51d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('HuggingFaceFW/fineweb-edu', name='sample-10BT', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7950f9-8ff3-40f4-b059-da2796d0a2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: ['text', 'id', 'dump', 'url', 'file_path', 'language', 'language_score', 'token_count', 'score', 'int_score'],\n",
       "    num_shards: 14\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8196360-e59b-4ce2-bbcd-c012f0ae1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Below is an extract from a web page. You are an AI content evaluator focused on assessing educational material's value for soft skills development. Soft skills include conversational ability, empathy, leadership skills, public speaking, confidence building, critical thinking, problem solving, professional writing, teamwork, digital literacy, professional attitude, work ethic, career management and intercultural fluency. \n",
    "\n",
    "You will analyze content using the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion:\n",
    "- Add 1 point if the extract hows superficial coverage of basic communication and teamwork concepts without meaningful depth or practical application. Professional development opportunities are limited to theoretical knowledge, and problem-solving scenarios lack complexity or real-world context. Cultural awareness and digital literacy elements are either absent or extremely basic.\n",
    "- Add another point if the extract specifically includes discussion of soft skills and includes straightforward communication scenarios and simple team dynamics, but lacks nuanced interaction or complex problem-solving opportunities. Professional development focuses on fundamental skills with limited practical application, while cultural awareness and digital literacy are present but superficial.\n",
    "- Award a third point if the extract specifically includes discussion of soft skills andfeatures realistic scenarios that integrate emotional intelligence, leadership challenges, and critical thinking opportunities. Professional development includes practical applications with meaningful context, while incorporating cultural awareness and modern digital literacy skills throughout the material. \n",
    "- Grant a fourth point if the extract specifically includes discussion of soft skills and presents complex scenarios requiring sophisticated communication, strategic thinking, and advanced problem-solving across multiple contexts. Professional development opportunities are comprehensive and practical, with strong emphasis on intercultural fluency and technological adaptation.\n",
    "- Bestow a fifth point if the extract specifically includes discussion of soft skills and seamlessly integrates advanced communication, leadership, and problem-solving scenarios that mirror real-world complexity. Professional development opportunities span multiple contexts with sophisticated cultural awareness, while digital literacy and practical application are woven throughout every element.\n",
    "\n",
    "After examining the extract: \n",
    "- Briefly justify your total score, up to 100 words.\n",
    "- Conclude with the score using the format: \"Educational score: <total points>\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5078a1c4-c45f-4ff8-9c80-a0e6b96a5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a56b4b-5c8c-4f2f-ac1f-9c68337d7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-15 06:41:26 config.py:510] This model supports multiple tasks: {'score', 'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 01-15 06:41:26 config.py:1310] Defaulting to use mp for distributed inference\n",
      "INFO 01-15 06:41:26 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 01-15 06:41:26 multiproc_worker_utils.py:312] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 01-15 06:41:26 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 01-15 06:41:27 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:41:28 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:41:28 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:28 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m INFO 01-15 06:41:28 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-15 06:41:28 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:28 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 01-15 06:41:33 utils.py:918] Found nccl from library libnccl.so.2\n",
      "INFO 01-15 06:41:33 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:33 utils.py:918] Found nccl from library libnccl.so.2\n",
      "INFO 01-15 06:41:33 utils.py:918] Found nccl from library libnccl.so.2\n",
      "INFO 01-15 06:41:33 utils.py:918] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:41:33 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:33 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 01-15 06:41:33 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 01-15 06:41:36 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:36 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 01-15 06:41:36 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 01-15 06:41:36 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 01-15 06:41:36 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_98c6ce09'), local_subscribe_port=60189, remote_subscribe_port=None)\n",
      "INFO 01-15 06:41:36 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m INFO 01-15 06:41:36 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...\n",
      "INFO 01-15 06:41:36 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...\n",
      "INFO 01-15 06:41:36 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:37 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:41:37 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "INFO 01-15 06:41:37 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m INFO 01-15 06:41:37 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8620edd26435420abc2b6b35c945396c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:41:48 model_runner.py:1099] Loading model weights took 32.8892 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m INFO 01-15 06:41:48 model_runner.py:1099] Loading model weights took 32.8892 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:48 model_runner.py:1099] Loading model weights took 32.8892 GB\n",
      "INFO 01-15 06:41:48 model_runner.py:1099] Loading model weights took 32.8892 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m INFO 01-15 06:41:53 worker.py:241] Memory profiling takes 4.45 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m INFO 01-15 06:41:53 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m INFO 01-15 06:41:53 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 3.70GiB; PyTorch activation peak memory takes 1.50GiB; the rest of the memory reserved for KV Cache is 98.83GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:41:53 worker.py:241] Memory profiling takes 4.45 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:41:53 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:41:53 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 4.17GiB; PyTorch activation peak memory takes 1.50GiB; the rest of the memory reserved for KV Cache is 98.36GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:53 worker.py:241] Memory profiling takes 4.48 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:53 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:53 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 4.17GiB; PyTorch activation peak memory takes 1.50GiB; the rest of the memory reserved for KV Cache is 98.36GiB.\n",
      "INFO 01-15 06:41:53 worker.py:241] Memory profiling takes 4.87 seconds\n",
      "INFO 01-15 06:41:53 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 01-15 06:41:53 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 4.96GiB; PyTorch activation peak memory takes 1.50GiB; the rest of the memory reserved for KV Cache is 97.58GiB.\n",
      "INFO 01-15 06:41:54 distributed_gpu_executor.py:57] # GPU blocks: 79936, # CPU blocks: 3276\n",
      "INFO 01-15 06:41:54 distributed_gpu_executor.py:61] Maximum concurrency for 16384 tokens per request: 78.06x\n",
      "INFO 01-15 06:41:58 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0% 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:41:58 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m INFO 01-15 06:41:58 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:41:58 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  97% 34/35 [00:22<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m INFO 01-15 06:42:20 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:42:21 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100% 35/35 [00:23<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-15 06:42:21 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:42:21 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16615)\u001b[0;0m INFO 01-15 06:42:21 model_runner.py:1535] Graph capturing finished in 24 secs, took 0.45 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16614)\u001b[0;0m INFO 01-15 06:42:21 model_runner.py:1535] Graph capturing finished in 24 secs, took 0.45 GiB\n",
      "INFO 01-15 06:42:21 model_runner.py:1535] Graph capturing finished in 24 secs, took 0.45 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16613)\u001b[0;0m INFO 01-15 06:42:21 model_runner.py:1535] Graph capturing finished in 23 secs, took 0.45 GiB\n",
      "INFO 01-15 06:42:21 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 32.88 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=16384, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bfe176e-8851-4cae-a207-b72246aaef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'Educational score: (\\d+)\\s*$'\n",
    "def extract_score(text):\n",
    "    match = re.search(pattern, text)\n",
    "    score = 0\n",
    "    if match:\n",
    "        score = int(match.group(1))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be81ccc6-6107-4b54-bf44-5749ea2d684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(texts):\n",
    "    messages = [[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"Extract:\\n\" + text[:50000] + \"\\n\\nJustification:\"}] for text in texts]\n",
    "    outputs = llm.chat(messages, SamplingParams(temperature=0.25, top_p=0.9, max_tokens=256))\n",
    "    responses = [output.outputs[0].text.strip() for output in outputs]\n",
    "    scores =  [extract_score(response) for response in responses]\n",
    "    return {\"justification\": responses, \"classification_score\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a643f2-4a78-46b4-9812-355dcea11da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating auto-logging. Current session state plus future input saved.\n",
      "Filename       : classify_progress.log\n",
      "Mode           : backup\n",
      "Output logging : False\n",
      "Raw input log  : True\n",
      "Timestamping   : True\n",
      "State          : active\n"
     ]
    }
   ],
   "source": [
    "%logstart -rt classify_progress.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3ebd8-3b3c-40c0-9fa8-c4810f10cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step: 0\n",
      "INFO 01-15 06:42:24 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 10000/10000 [20:27<00:00,  8.15it/s, est. speed input: 11653.59 toks/s, output: 490.80 toks/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1339321e284e80aa527b80863db2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 10000/10000 [20:32<00:00,  8.11it/s, est. speed input: 11658.43 toks/s, output: 497.48 toks/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3e41ed936c440b87727bb53ba579a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0% 22/10000 [00:37<52:53,  3.14it/s, est. speed input: 602.70 toks/s, output: 24.68 toks/s]  "
     ]
    }
   ],
   "source": [
    "total_count = 1000 * 1000\n",
    "num_steps = 100\n",
    "step_size = total_count // num_steps\n",
    "for step in range(num_steps):\n",
    "    print(f\"Running step: {step}\")\n",
    "    \n",
    "    cls_ds_stream = ds.take(step_size).map(classify, batched=True, batch_size=step_size, input_columns=[\"text\"])\n",
    "    cls_ds_list = list(cls_ds_stream)\n",
    "    \n",
    "    cls_ds = Dataset.from_list(cls_ds_list)\n",
    "    cls_ds.save_to_disk(f\"soft-skills-cls-{step}.hf\")\n",
    "\n",
    "    ds = ds.skip(step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ebcb2-e0f6-412d-92da-6d381e3fa4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%logstop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f2329-05e7-4801-b565-bbf7658ee736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
